import torch
#from hitsgnn import HITSGNN
from hitsgnn_rev2 import HITSGNN
from utils import *
import argparse
import numpy as np
from metattack import MetaApprox, Metattack
import torch.nn.functional as F
import torch.optim as optim
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd

parser = argparse.ArgumentParser()
parser.add_argument('--seed', type=int, default=17, help='Random seed.')
parser.add_argument('--epochs', type=int, default=200,
                    help='Number of epochs to train.')
parser.add_argument('--lr', type=float, default=0.01,
                    help='Initial learning rate.')
parser.add_argument('--hidden', type=int, default=16,
                    help='Number of hidden units.')
parser.add_argument('--dataset', type=str, default='citeseer',
                    choices=['cora', 'cora_ml', 'citeseer', 'polblogs'], help='dataset')
parser.add_argument('--ptb_rate', type=float, default=0.05,  help='pertubation rate')
parser.add_argument('--model', type=str, default='Meta-Self', choices=['A-Meta-Self', 'Meta-Self'], help='model variant')

args = parser.parse_args()
cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

np.random.seed(args.seed)
torch.manual_seed(args.seed)
if device != 'cpu':
    torch.cuda.manual_seed(args.seed)

# === loading dataset
adj, features, labels = load_data(dataset=args.dataset)
nclass = max(labels) + 1

val_size = 0.1
test_size = 0.8
train_size = 1 - test_size - val_size

idx = np.arange(adj.shape[0])
idx_train, idx_val, idx_test = get_train_val_test(idx, train_size, val_size, test_size, stratify=labels)


print("idx_test", type(idx_test))



idx_test1=[1018, 519, 2055, 1544, 2058, 522, 2056, 1036, 1037, 527, 1552, 528, 2063, 2065, 2068, 524, 2071, 2073, 536, 1563, 541, 1569, 2083, 548, 549, 1575, 2088, 551, 1067, 1068, 2093, 558, 554, 1576, 2098, 562, 1076, 564, 1074, 1081, 1082, 59, 572, 1596, 2109, 576, 71, 588, 80, 595, 1108, 598, 1110, 526, 95, 1632, 1634, 613, 1127, 1136, 628, 117, 119, 2059, 633, 2060, 1662, 1663, 640, 2061, 132, 1670, 646, 1674, 650, 140, 144, 149, 1686, 151, 1687, 1689, 155, 1182, 158, 1699, 1190, 682, 683, 177, 181, 1717, 694, 186, 702, 1727, 709, 198, 1223, 1058, 729, 1246, 1760, 1761, 1252, 1772, 236, 1260, 244, 756, 1780, 762, 261, 1798, 266, 1803, 268, 1806, 1296, 1297, 1299, 787, 791, 1815, 1817, 1307, 797, 1821, 289, 1316, 292, 1319, 804, 805, 295, 1318, 304, 305, 1849, 314, 1337, 1852, 1342, 1343, 1859, 1860, 1862, 841, 843, 844, 846, 1871, 336, 343, 345, 346, 1372, 350, 352, 866, 355, 1891, 359, 1896, 873, 1899, 875, 877, 878, 880, 364, 1399, 376, 1917, 1410, 1926, 1928, 393, 394, 1930, 1931, 397, 1414, 404, 1429, 1428, 1436, 414, 1438, 417, 1446, 423, 937, 1452, 1965, 434, 1972, 1464, 441, 1979, 444, 1468, 1981, 1469, 448, 445, 963, 1476, 455, 970, 1998, 1487, 464, 2000, 981, 1494, 476, 1501, 2017, 995, 1510, 1511, 1000, 489, 1003, 2027, 495, 2031, 499, 2036, 1523, 500, 505, 506, 509]

idx_test2=[0, 1, 2051, 2052, 2055, 6, 2056, 8, 2058, 2061, 2062, 14, 19, 2068, 21, 24, 2074, 30, 2083, 36, 2084, 39, 38, 37, 2090, 41, 2087, 45, 50, 52, 56, 62, 63, 66, 70, 71, 73, 75, 76, 77, 78, 80, 83, 88, 89, 100, 102, 105, 113, 114, 121, 125, 132, 144, 156, 159, 162, 176, 181, 186, 188, 189, 198, 207, 218, 224, 244, 247, 253, 254, 266, 270, 284, 289, 293, 302, 308, 315, 317, 322, 327, 337, 354, 355, 361, 362, 388, 390, 415, 420, 436, 437, 447, 452, 473, 511, 516, 560, 589, 595, 598, 601, 602, 607, 608, 609, 612, 614, 619, 620, 630, 631, 636, 638, 640, 645, 652, 653, 655, 657, 662, 668, 669, 684, 685, 686, 696, 704, 709, 710, 718, 721, 725, 732, 740, 746, 751, 757, 759, 764, 765, 766, 780, 783, 787, 791, 797, 815, 817, 818, 823, 831, 832, 833, 834, 839, 842, 844, 848, 859, 860, 862, 864, 871, 875, 879, 880, 885, 886, 888, 896, 897, 898, 904, 906, 908, 915, 922, 927, 938, 941, 943, 945, 950, 954, 962, 974, 979, 983, 987, 989, 992, 999, 1001, 1003, 1012, 1013, 1018, 1019, 1021, 1023, 1026, 1033, 1036, 1038, 1051, 1063, 1075, 1077, 1079, 1080, 1082, 1083, 1109, 1110, 1132, 1140, 1144, 1148, 1155, 1159, 1160, 1164, 1167, 1171, 1180, 1191, 1193, 1196, 1200, 1203, 1210, 1220, 1224, 1227, 1234, 1246, 1251, 1255, 1256, 1262, 1273, 1280, 1282, 1283, 1284, 1285, 1287, 1289, 1295, 1296, 1297, 1298, 1302, 1305, 1307, 1314, 1317, 1324, 1333, 1335, 1337, 1343, 1348, 1350, 1354, 1355, 1362, 1364, 1368, 1373, 1387, 1388, 1390, 1391, 1392, 1393, 1404, 1410, 1419, 1425, 1428, 1433, 1437, 1439, 1445, 1451, 1453, 1454, 1456, 1458, 1468, 1476, 1485, 1487, 1507, 1509, 1512, 1515, 1517, 1520, 1521, 1526, 1531, 1536, 1537, 1538, 1541, 1544, 1548, 1553, 1554, 1564, 1566, 1572, 1577, 1584, 1587, 1588, 1603, 1604, 1612, 1632, 1635, 1637, 1649, 1654, 1661, 1665, 1671, 1675, 1684, 1687, 1690, 1694, 1695, 1706, 1708, 1718, 1721, 1722, 1724, 1733, 1741, 1750, 1753, 1770, 1773, 1790, 1793, 1797, 1798, 1800, 1803, 1805, 1809, 1813, 1818, 1820, 1829, 1832, 1833, 1847, 1851, 1859, 1864, 1868, 1874, 1877, 1878, 1894, 1908, 1910, 1913, 1920, 1923, 1927, 1933, 1940, 1951, 1954, 1955, 1970, 1978, 1980, 1983, 1999, 2017, 2019, 2023, 2029, 2030, 2036, 2040, 2042, 2043]

idx_test3=[2049, 2, 2056, 2059, 2060, 2062, 2063, 2064, 17, 2065, 2067, 2068, 16, 14, 2066, 25, 2073, 2075, 2076, 2078, 2079, 2081, 2082, 2083, 2084, 2088, 2089, 2093, 2094, 45, 2098, 2096, 53, 2101, 56, 57, 2104, 2107, 2105, 2109, 63, 64, 72, 74, 79, 80, 83, 87, 88, 106, 113, 114, 119, 120, 121, 125, 133, 142, 144, 145, 149, 151, 155, 158, 161, 162, 165, 168, 169, 173, 174, 176, 180, 181, 186, 198, 199, 200, 202, 210, 212, 213, 215, 216, 218, 228, 233, 237, 238, 239, 240, 246, 247, 250, 261, 266, 268, 271, 272, 274, 278, 282, 286, 294, 295, 304, 305, 307, 314, 316, 328, 329, 330, 336, 343, 345, 346, 347, 348, 349, 350, 351, 354, 355, 356, 357, 359, 367, 370, 372, 380, 381, 382, 385, 387, 388, 393, 394, 398, 399, 400, 403, 404, 408, 409, 410, 414, 415, 420, 421, 422, 423, 431, 434, 441, 442, 444, 445, 447, 448, 449, 457, 459, 461, 464, 465, 466, 468, 471, 472, 476, 478, 482, 486, 487, 489, 493, 497, 502, 503, 506, 507, 513, 514, 515, 518, 519, 520, 523, 524, 532, 535, 536, 538, 540, 542, 543, 544, 547, 548, 550, 551, 552, 554, 558, 559, 569, 572, 578, 581, 584, 594, 595, 598, 605, 613, 614, 620, 625, 626, 628, 629, 631, 634, 639, 640, 642, 646, 647, 650, 652, 658, 660, 663, 668, 673, 675, 684, 685, 691, 692, 702, 704, 705, 709, 710, 717, 718, 719, 731, 735, 736, 737, 738, 739, 740, 746, 755, 756, 759, 761, 789, 791, 794, 799, 804, 805, 813, 815, 817, 818, 819, 820, 821, 833, 836, 839, 843, 844, 846, 847, 852, 858, 862, 863, 866, 869, 870, 873, 874, 875, 887, 890, 892, 893, 897, 898, 903, 904, 907, 912, 923, 928, 933, 935, 937, 948, 950, 952, 956, 957, 958, 959, 961, 962, 963, 967, 970, 974, 975, 976, 977, 980, 981, 986, 987, 989, 992, 999, 1000, 1003, 1006, 1007, 1008, 1011, 1018, 1019, 1021, 1022, 1023, 1026, 1028, 1036, 1038, 1044, 1045, 1048, 1052, 1056, 1058, 1059, 1067, 1068, 1074, 1075, 1076, 1077, 1081, 1082, 1085, 1089, 1095, 1096, 1100, 1101, 1110, 1112, 1113, 1123, 1127, 1132, 1136, 1138, 1139, 1142, 1143, 1145, 1147, 1151, 1155, 1161, 1164, 1165, 1168, 1169, 1170, 1174, 1180, 1182, 1190, 1191, 1192, 1193, 1196, 1197, 1199, 1206, 1208, 1214, 1216, 1220, 1221, 1223, 1226, 1227, 1238, 1241, 1245, 1246, 1247, 1248, 1249, 1252, 1254, 1259, 1260, 1261, 1275, 1276, 1295, 1299, 1306, 1312, 1319, 1330, 1331, 1333, 1334, 1335, 1337, 1342, 1343, 1345, 1346, 1348, 1351, 1355, 1361, 1363, 1370, 1385, 1386, 1390, 1396, 1400, 1401, 1405, 1407, 1408, 1410, 1413, 1419, 1423, 1425, 1428, 1429, 1440, 1442, 1444, 1446, 1451, 1452, 1453, 1456, 1459, 1462, 1464, 1468, 1469, 1470, 1475, 1476, 1480, 1486, 1490, 1494, 1502, 1503, 1509, 1510, 1511, 1515, 1516, 1523, 1524, 1526, 1533, 1540, 1545, 1548, 1550, 1552, 1553, 1554, 1566, 1567, 1569, 1576, 1584, 1588, 1589, 1596, 1601, 1603, 1604, 1609, 1612, 1614, 1617, 1619, 1627, 1628, 1634, 1635, 1641, 1647, 1651, 1653, 1657, 1662, 1663, 1665, 1666, 1669, 1670, 1671, 1674, 1680, 1686, 1687, 1689, 1699, 1702, 1712, 1713, 1717, 1720, 1721, 1724, 1728, 1729, 1732, 1733, 1736, 1739, 1740, 1741, 1746, 1747, 1753, 1759, 1760, 1761, 1762, 1771, 1775, 1781, 1783, 1784, 1786, 1787, 1788, 1798, 1801, 1802, 1803, 1805, 1806, 1808, 1812, 1815, 1817, 1818, 1819, 1821, 1823, 1830, 1832, 1833, 1836, 1837, 1842, 1848, 1853, 1856, 1861, 1862, 1866, 1868, 1871, 1873, 1875, 1876, 1891, 1892, 1893, 1895, 1897, 1898, 1899, 1903, 1905, 1906, 1929, 1930, 1934, 1935, 1938, 1941, 1942, 1946, 1949, 1952, 1960, 1964, 1965, 1972, 1973, 1979, 1981, 1989, 1994, 1997, 1999, 2000, 2001, 2004, 2031, 2032, 2033, 2038, 2041]

idx_test4=[3, 516, 6, 2056, 1544, 10, 1547, 7, 1038, 16, 1554, 21, 2076, 1059, 1572, 1573, 38, 37, 41, 2090, 2043, 36, 45, 2097, 1078, 1079, 56, 66, 1603, 1604, 71, 75, 78, 79, 82, 595, 598, 601, 602, 604, 1119, 1638, 102, 614, 1127, 620, 1132, 114, 631, 1655, 1144, 1665, 129, 1155, 644, 1159, 1160, 652, 1164, 655, 1680, 658, 1173, 1687, 1689, 1180, 679, 167, 1706, 1191, 1196, 1210, 1227, 207, 721, 209, 1750, 1752, 1242, 737, 1251, 740, 1767, 1770, 1262, 245, 759, 1273, 1285, 1286, 1288, 1289, 1298, 1302, 1820, 800, 803, 1829, 304, 817, 1333, 1334, 1335, 823, 826, 315, 317, 314, 1855, 832, 831, 1348, 842, 1357, 1872, 848, 1364, 1367, 1368, 858, 860, 862, 1887, 1378, 868, 869, 1894, 873, 362, 1388, 1389, 1392, 1905, 1394, 1908, 885, 1910, 1407, 1408, 896, 914, 915, 1952, 1954, 931, 938, 1963, 941, 1454, 1453, 1463, 1978, 956, 957, 447, 960, 1983, 1980, 452, 987, 1508, 1512, 1001, 1517, 1010, 2038, 1531]

idx_test5=[2055, 2056, 2059, 2060, 2061, 2068, 2069, 2084, 2086, 2088, 2089, 2093, 52, 53, 56, 2108, 2109, 72, 80, 83, 95, 100, 105, 106, 113, 119, 121, 129, 132, 144, 151, 155, 181, 186, 187, 188, 209, 215, 236, 244, 248, 266, 267, 268, 269, 277, 284, 286, 305, 327, 330, 336, 350, 354, 355, 357, 360, 390, 393, 404, 410, 413, 414, 420, 421, 423, 428, 436, 442, 444, 448, 455, 464, 471, 476, 487, 490, 505, 509, 512, 519, 524, 527, 532, 543, 550, 558, 576, 590, 595, 613, 614, 625, 627, 628, 646, 650, 661, 683, 684, 702, 705, 711, 756, 764, 766, 788, 791, 798, 815, 819, 820, 821, 839, 841, 843, 844, 865, 866, 871, 877, 878, 879, 892, 898, 914, 923, 927, 937, 948, 970, 987, 1008, 1015, 1016, 1018, 1023, 1026, 1028, 1031, 1034, 1035, 1038, 1058, 1067, 1071, 1080, 1081, 1082, 1091, 1094, 1100, 1108, 1110, 1113, 1155, 1168, 1170, 1174, 1196, 1197, 1201, 1208, 1217, 1222, 1223, 1244, 1245, 1246, 1252, 1260, 1280, 1295, 1296, 1297, 1298, 1299, 1304, 1306, 1342, 1344, 1358, 1365, 1370, 1382, 1383, 1400, 1423, 1429, 1442, 1452, 1464, 1475, 1480, 1486, 1490, 1509, 1510, 1511, 1520, 1522, 1523, 1526, 1563, 1569, 1583, 1596, 1604, 1609, 1612, 1621, 1631, 1651, 1663, 1674, 1677, 1686, 1687, 1689, 1701, 1713, 1717, 1724, 1727, 1733, 1753, 1759, 1806, 1810, 1811, 1819, 1823, 1830, 1832, 1838, 1851, 1852, 1853, 1862, 1871, 1881, 1883, 1889, 1891, 1896, 1918, 1919, 1928, 1930, 1937, 1938, 1939, 1966, 1972, 1977, 1980, 1981, 2000, 2007, 2022, 2027, 2030, 2031, 2036, 2040]

idx_test6=[514, 6, 2055, 8, 521, 1547, 11, 2062, 1554, 1559, 1562, 1051, 1564, 1567, 2084, 39, 1575, 38, 42, 2087, 1584, 1585, 2097, 1587, 2099, 2096, 1591, 56, 1083, 62, 1598, 1088, 1090, 1603, 1091, 1604, 1609, 74, 75, 82, 1620, 598, 600, 89, 90, 92, 1120, 1635, 1124, 1637, 1132, 1644, 1138, 114, 1140, 1141, 630, 631, 1144, 121, 1657, 632, 636, 1661, 1151, 640, 1665, 1155, 1160, 648, 138, 652, 654, 655, 658, 1171, 1684, 662, 155, 1180, 668, 1694, 1695, 438, 162, 167, 1191, 1193, 1706, 1196, 1197, 686, 174, 176, 1210, 1215, 704, 1227, 1741, 207, 1229, 721, 724, 1750, 1242, 1253, 1767, 1255, 1258, 1770, 236, 1777, 245, 1273, 1787, 765, 1790, 1280, 1794, 771, 1283, 1286, 1289, 1803, 783, 1808, 784, 1298, 1299, 1302, 1814, 1816, 794, 1819, 1820, 1821, 802, 293, 1835, 1838, 817, 1842, 308, 1333, 1337, 1341, 1855, 832, 1859, 1348, 839, 1352, 1355, 1872, 1362, 1874, 1877, 1366, 1368, 862, 354, 1379, 1378, 869, 1894, 870, 1389, 1390, 1904, 881, 1392, 1908, 1910, 375, 888, 893, 1407, 1408, 896, 898, 1410, 904, 1419, 1933, 1425, 1428, 916, 404, 1943, 1950, 1952, 1443, 931, 938, 427, 941, 1454, 1453, 1974, 1973, 1461, 1978, 952, 956, 957, 447, 1980, 953, 962, 452, 974, 1999, 2001, 1491, 979, 1497, 987, 989, 993, 2020, 1508, 1001, 1003, 1010, 2040, 1530, 508, 1021]

idx_test7=[6, 8, 521, 1547, 11, 1559, 1562, 1051, 1564, 39, 38, 42, 2087, 1585, 2097, 1587, 2099, 1591, 1083, 62, 1598, 1088, 1090, 1091, 75, 82, 1620, 600, 89, 90, 92, 1120, 1124, 1637, 1644, 1140, 1141, 630, 1144, 632, 636, 1661, 1160, 648, 138, 654, 655, 1171, 1684, 662, 1694, 1695, 438, 167, 1706, 686, 1210, 1215, 207, 1229, 721, 724, 1750, 1242, 1253, 1767, 1255, 1258, 1770, 1777, 245, 1273, 765, 1790, 1280, 1794, 771, 1283, 1286, 1289, 783, 784, 1298, 1302, 1814, 1816, 1820, 802, 293, 1835, 1838, 308, 1341, 1855, 832, 1352, 1872, 1362, 1874, 1877, 1366, 1368, 1379, 1378, 1894, 1389, 1904, 881, 1392, 1908, 1910, 375, 888, 896, 1933, 916, 1943, 1950, 1443, 931, 938, 427, 941, 1454, 1974, 1461, 1978, 1980, 953, 452, 1491, 979, 1497, 993, 2020, 1508, 1001, 1010, 2040, 1530, 508]
#print("adj.shape[0]",adj.shape[0])
#idx_test8=[3, 516, 6, 10, 1547, 7, 21, 1572, 1573, 38, 37, 41, 2090, 2043, 36, 2097, 1078, 1079, 66, 75, 78, 82, 601, 602, 604, 1119, 1638, 102, 1655, 1144, 129, 644, 1159, 1160, 655, 1173, 679, 167, 1706, 1210, 207, 721, 209, 1750, 1752, 1242, 1251, 1767, 1770, 1262, 245, 1273, 1285, 1286, 1288, 1289, 1298, 1302, 1820, 800, 803, 1829, 823, 826, 315, 317, 1855, 832, 831, 842, 1357, 1872, 848, 1364, 1367, 1368, 860, 1887, 1378, 868, 1894, 362, 1388, 1389, 1392, 1394, 1908, 885, 1910, 896, 914, 915, 1954, 931, 938, 1963, 941, 1454, 1463, 1978, 960, 1983, 1980, 452, 1508, 1512, 1001, 1517, 1010, 1531]

idx_test8=[1018, 519, 2058, 522, 1036, 1037, 527, 1552, 528, 2063, 2065, 2068, 524, 2071, 2073, 536, 1563, 541, 1569, 2083, 548, 549, 2088, 551, 1067, 1068, 2093, 558, 554, 1576, 2098, 562, 1076, 564, 1074, 1081, 1082, 59, 572, 1596, 2109, 576, 588, 80, 1108, 1110, 526, 95, 1632, 1634, 613, 1136, 628, 117, 119, 2059, 633, 2060, 1662, 1663, 2061, 132, 1670, 646, 1674, 650, 140, 144, 149, 1686, 151, 1182, 158, 1699, 1190, 682, 683, 177, 181, 1717, 694, 186, 702, 1727, 709, 198, 1223, 1058, 729, 1246, 1760, 1761, 1252, 1772, 1260, 244, 756, 1780, 762, 261, 1798, 266, 268, 1806, 1296, 1297, 787, 791, 1815, 1817, 1307, 797, 289, 1316, 292, 1319, 804, 805, 295, 1318, 305, 1849, 1852, 1342, 1343, 1860, 1862, 841, 843, 844, 846, 1871, 336, 343, 345, 346, 1372, 350, 352, 866, 355, 1891, 359, 1896, 1899, 875, 877, 878, 880, 364, 1399, 376, 1917, 1926, 1928, 393, 394, 1930, 1931, 397, 1414, 1429, 1436, 414, 1438, 417, 1446, 423, 937, 1452, 1965, 434, 1972, 1464, 441, 1979, 444, 1468, 1981, 1469, 448, 445, 963, 1476, 455, 970, 1998, 1487, 464, 2000, 981, 1494, 476, 1501, 2017, 995, 1510, 1511, 1000, 489, 2027, 495, 2031, 499, 2036, 1523, 500, 505, 506, 509]


idx_test=np.array(idx_test8)

#idx_test=idx_test1

idx_unlabeled = np.union1d(idx_val, idx_test)





perturbations = int(args.ptb_rate * (adj.sum()//2))

adj, features, labels = preprocess(adj, features, labels, preprocess_adj=False)



# set up attack model
if 'Self' in args.model:
    lambda_ = 0
if 'Train' in args.model:
    lambda_ = 1
if 'Both' in args.model:
    lambda_ = 0.5

if 'A' in args.model:
    model = MetaApprox(nfeat=features.shape[1], hidden_sizes=[args.hidden],
                       nnodes=adj.shape[0], nclass=nclass, dropout=0.5,
                       train_iters=100, attack_features=False, lambda_=lambda_, device=device)

else:
    model = Metattack(nfeat=features.shape[1], hidden_sizes=[args.hidden],
                       nnodes=adj.shape[0], nclass=nclass, dropout=0.5,
                       train_iters=100, attack_features=False, lambda_=lambda_, device=device)

if device != 'cpu':
    adj = adj.to(device)
    features = features.to(device)
    labels = labels.to(device)
    model = model.to(device)


def test(adj):
    ''' test on HITSGNN '''

    adj = normalize_adj_tensor(adj)
    hitsgnn = HITSGNN(nfeat=features.shape[1],
              nhid=args.hidden,
              nclass=labels.max().item() + 1,
              dropout=0.5)

    if device != 'cpu':
        hitsgnn = hitsgnn.to(device)

    optimizer = optim.Adam(hitsgnn.parameters(),
                           lr=args.lr, weight_decay=5e-4)

    hitsgnn.train()

    for epoch in range(args.epochs):
        optimizer.zero_grad()
        output = hitsgnn(features, adj)
        loss_train = F.nll_loss(output[idx_train], labels[idx_train])
        acc_train = accuracy(output[idx_train], labels[idx_train])
        loss_train.backward()
        optimizer.step()

    hitsgnn.eval()
    output = hitsgnn(features, adj)


    loss_test = F.nll_loss(output[idx_test], labels[idx_test])
    acc_test = accuracy(output[idx_test], labels[idx_test])


    # print("Test set results:",
    #       "loss= {:.4f}".format(loss_test.item()),
    #       "accuracy= {:.4f}".format(acc_test.item()))

    return acc_test.item()


def main():

    acc = []
    runs = 1
    
    for i in range(runs):
        acc.append(test(adj))

    torch.cuda.empty_cache()

    #for i in range(3):
    
    modified_adj = model(features, adj, labels, idx_train,
                            idx_unlabeled, perturbations, ll_constraint=False)
    modified_adj = modified_adj.detach()

    for i in range(runs):
        acc.append(test(modified_adj))

    print("acc",acc)



    #data=pd.DataFrame({"Acc. Clean":clean_acc,"Acc. Perturbed":attacked_acc})

    #plt.figure(figsize=(6,6))
    #sns.boxplot(data=data)#, re_trainings*[accuracy_logistic]])

    #plt.title("Accuracy before/after perturbing {}% edges using model {}".format(args.ptb_rate*100, args.model))
    #plt.savefig("results_on_{}.png".format(args.dataset), dpi=600)
    #plt.show()


if __name__ == '__main__':
    main()

#neighbor type 0:
#acc [0.7049763033175356, 0.5645734597156399, 0.5414691943127963, 0.5485781990521328]
#acc [0.7049763033175356, 0.5882701421800949, 0.5112559241706162, 0.4893364928909953]

#neighbor type 1:
#acc [0.7446808510638298, 0.574468085106383,  0.5276595744680851, 0.5531914893617021]
#acc [0.7446808510638298, 0.548936170212766, 0.5063829787234042, 0.5106382978723404, ]

#neighbor type 2:
#acc [0.7222222222222223, 0.6590909090909092, 0.6792929292929294, 0.6616161616161617]
#acc [0.7222222222222223, 0.6641414141414141, 0.6666666666666667, 0.6641414141414141]

#neighbor type 3:
#acc [0.7689873417721519, 0.6582278481012658, 0.6424050632911392, 0.6534810126582279]
#acc [0.7689873417721519, 0.6123417721518988, 0.6107594936708861, 0.625]

#neighbor type 4:
#acc [0.718562874251497, 0.6946107784431138, 0.7005988023952097, 0.6766467065868264]
#acc [0.718562874251497, 0.6526946107784432, 0.6706586826347306, 0.6646706586826348]

#neighbor type 5:
#acc [0.7132075471698113, 0.7320754716981133, 0.739622641509434, 0.7358490566037736]
#acc [0.7132075471698113, 0.7132075471698113, 0.7169811320754716, 0.7169811320754716]

#neighbor type 6:
#acc [0.7543859649122806, 0.7456140350877193, 0.7368421052631579, 0.75]
#acc [0.7543859649122806, 0.75, 0.7587719298245613, 0.7280701754385964]

#neighbor type7:
#acc [0.7872340425531915, 0.7163120567375887, 0.6879432624113475, 0.6808510638297872]

#neighbor type8:
#acc [0.7636363636363636, 0.7, 0.6545454545454545, 0.6545454545454545]

#neighbor type8-0:
#acc [0.7666666666666667, 0.5761904761904763, 0.5047619047619049, 0.5285714285714286]
#acc [0.7666666666666667, 0.5333333333333334, 0.5, 0.5428571428571429]


