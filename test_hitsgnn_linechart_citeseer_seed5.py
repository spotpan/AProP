import torch
#from hitsgnn import HITSGNN
from hitsgnn_rev2 import HITSGNN
from utils import *
import argparse
import numpy as np
from metattack import MetaApprox, Metattack
import torch.nn.functional as F
import torch.optim as optim
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd

parser = argparse.ArgumentParser()
parser.add_argument('--seed', type=int, default=19, help='Random seed.')
parser.add_argument('--epochs', type=int, default=200,
                    help='Number of epochs to train.')
parser.add_argument('--lr', type=float, default=0.01,
                    help='Initial learning rate.')
parser.add_argument('--hidden', type=int, default=16,
                    help='Number of hidden units.')
parser.add_argument('--dataset', type=str, default='citeseer',
                    choices=['cora', 'cora_ml', 'citeseer', 'polblogs'], help='dataset')
parser.add_argument('--ptb_rate', type=float, default=0.05,  help='pertubation rate')
parser.add_argument('--model', type=str, default='Meta-Self', choices=['A-Meta-Self', 'Meta-Self'], help='model variant')

args = parser.parse_args()
cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

np.random.seed(args.seed)
torch.manual_seed(args.seed)
if device != 'cpu':
    torch.cuda.manual_seed(args.seed)

# === loading dataset
adj, features, labels = load_data(dataset=args.dataset)
nclass = max(labels) + 1

val_size = 0.1
test_size = 0.8
train_size = 1 - test_size - val_size

idx = np.arange(adj.shape[0])
idx_train, idx_val, idx_test = get_train_val_test(idx, train_size, val_size, test_size, stratify=labels)


print("idx_test", type(idx_test))


idx_test1=[2056, 1036, 1037, 527, 528, 1552, 530, 1045, 2071, 1569, 545, 2083, 548, 2085, 1063, 1576, 1571, 554, 1067, 551, 2093, 1071, 561, 562, 564, 1076, 567, 59, 572, 573, 1603, 1604, 582, 1098, 1613, 80, 595, 1621, 96, 1632, 614, 103, 106, 1130, 1642, 1645, 625, 1146, 1662, 128, 1668, 646, 1673, 1674, 139, 144, 151, 1689, 1180, 1699, 1187, 1190, 1705, 1194, 1195, 684, 1199, 175, 1717, 181, 1209, 702, 1727, 1728, 1729, 194, 192, 704, 710, 706, 203, 1735, 711, 215, 1754, 1757, 1247, 1248, 1762, 1252, 737, 233, 1259, 238, 752, 242, 1270, 247, 1788, 1796, 1798, 776, 266, 1803, 1293, 1806, 1299, 1301, 1815, 1304, 1821, 287, 291, 805, 1318, 807, 1831, 1832, 1830, 300, 1316, 1326, 304, 305, 1849, 314, 316, 1853, 1857, 1862, 1351, 843, 1871, 1875, 1363, 855, 346, 1374, 1889, 354, 1897, 1385, 877, 1390, 880, 370, 1909, 1399, 380, 382, 1413, 390, 392, 393, 394, 1925, 911, 400, 404, 1428, 923, 1436, 418, 422, 937, 1452, 1965, 1460, 1974, 441, 1466, 1979, 444, 1468, 448, 455, 1481, 1997, 1998, 1487, 1488, 464, 978, 1490, 1494, 475, 987, 988, 1504, 2015, 503, 996, 487, 1000, 1828, 1007, 1520, 496, 1522, 1523, 2037, 501, 505, 1018, 507, 2047]

idx_test2=[1, 2052, 4, 6, 2056, 10, 2061, 2062, 16, 19, 21, 23, 24, 2077, 2078, 31, 2080, 33, 34, 2083, 2085, 2087, 2079, 41, 2090, 2091, 2093, 48, 2097, 50, 2099, 51, 54, 63, 75, 76, 78, 83, 89, 91, 97, 98, 109, 110, 113, 114, 125, 144, 147, 153, 157, 171, 175, 176, 36, 184, 187, 188, 198, 200, 203, 205, 207, 211, 218, 220, 225, 2081, 247, 253, 264, 278, 297, 302, 314, 315, 317, 326, 327, 332, 337, 354, 362, 363, 390, 392, 425, 430, 451, 452, 485, 487, 488, 516, 537, 557, 560, 599, 601, 602, 603, 609, 612, 614, 615, 616, 618, 619, 622, 623, 624, 626, 638, 640, 649, 651, 653, 654, 655, 662, 669, 678, 688, 696, 700, 718, 720, 724, 725, 728, 737, 740, 745, 748, 760, 764, 765, 780, 784, 786, 795, 796, 803, 825, 827, 829, 830, 831, 838, 839, 840, 853, 857, 858, 860, 864, 867, 868, 876, 878, 879, 886, 891, 898, 904, 915, 922, 924, 925, 927, 929, 936, 938, 949, 950, 952, 953, 954, 968, 980, 982, 983, 987, 999, 1001, 1003, 1007, 1013, 1018, 1019, 1021, 1026, 1027, 1036, 1038, 1040, 1041, 1042, 1043, 1052, 1054, 1056, 1063, 1064, 1070, 1079, 1093, 1095, 1099, 1105, 1109, 1110, 1130, 1139, 1141, 1149, 1150, 1151, 1152, 1159, 1160, 1171, 1191, 1193, 1194, 1196, 1197, 1206, 1220, 1228, 1243, 1278, 1280, 1285, 1287, 1294, 1317, 1324, 1327, 1329, 1350, 1355, 1367, 1374, 1377, 1378, 1381, 1388, 1390, 1391, 1393, 1406, 1410, 1418, 1419, 1425, 1428, 1437, 1439, 1445, 1453, 1454, 1462, 1465, 1468, 1481, 1483, 1484, 1487, 1507, 1515, 1517, 1531, 1536, 1543, 1553, 1559, 1584, 1591, 1592, 1599, 1601, 1605, 1606, 1611, 1612, 1617, 1618, 1622, 1630, 1632, 1635, 1636, 1644, 1648, 1649, 1660, 1661, 1665, 1667, 1672, 1675, 1681, 1687, 1689, 1694, 1706, 1711, 1725, 1731, 1750, 1751, 1753, 1755, 1766, 1793, 1795, 1798, 1801, 1803, 1804, 1805, 1809, 1814, 1817, 1829, 1843, 1848, 1851, 1859, 1876, 1878, 1894, 1904, 1905, 1908, 1927, 1930, 1945, 1948, 1954, 1958, 1963, 1969, 1978, 1982, 2017, 2023, 2026, 2039]

idx_test3=[2049, 2056, 2059, 2060, 2062, 2064, 17, 2067, 2066, 2069, 23, 2072, 25, 2075, 2077, 2079, 2080, 2081, 34, 2085, 2083, 40, 2093, 2096, 49, 2100, 2101, 2102, 53, 57, 2107, 2108, 59, 2109, 63, 60, 67, 68, 71, 78, 79, 80, 16, 83, 86, 91, 18, 96, 97, 98, 103, 106, 107, 111, 113, 114, 116, 119, 120, 124, 133, 136, 139, 142, 143, 144, 145, 151, 155, 157, 158, 164, 168, 171, 174, 175, 176, 178, 181, 188, 192, 194, 196, 198, 199, 203, 213, 214, 215, 216, 218, 222, 225, 228, 232, 239, 242, 243, 246, 247, 248, 250, 253, 255, 261, 266, 269, 272, 274, 278, 279, 280, 282, 284, 285, 286, 287, 294, 295, 296, 299, 302, 305, 314, 316, 323, 327, 340, 346, 349, 354, 356, 358, 359, 361, 363, 369, 370, 376, 380, 381, 382, 383, 384, 387, 390, 393, 394, 398, 399, 401, 403, 410, 420, 421, 422, 429, 430, 431, 433, 440, 441, 442, 444, 448, 449, 450, 456, 457, 461, 465, 475, 476, 480, 484, 485, 487, 489, 491, 499, 503, 504, 506, 507, 514, 515, 524, 529, 530, 531, 535, 538, 540, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 553, 554, 559, 563, 564, 569, 570, 572, 575, 577, 578, 584, 592, 593, 595, 606, 613, 614, 618, 625, 626, 628, 631, 634, 638, 639, 642, 645, 646, 651, 652, 661, 668, 670, 683, 684, 689, 691, 693, 695, 702, 710, 712, 716, 717, 718, 721, 729, 731, 735, 737, 740, 742, 752, 755, 756, 758, 764, 777, 786, 791, 792, 795, 797, 799, 802, 803, 805, 806, 807, 810, 813, 815, 816, 818, 819, 820, 821, 828, 833, 836, 839, 841, 843, 845, 850, 852, 854, 855, 857, 865, 876, 877, 878, 879, 880, 884, 887, 892, 894, 898, 902, 904, 908, 912, 916, 918, 919, 920, 921, 929, 930, 931, 933, 937, 946, 948, 950, 961, 962, 963, 966, 967, 970, 976, 978, 980, 987, 988, 989, 991, 996, 999, 1000, 1007, 1008, 1017, 1018, 1019, 1021, 1023, 1026, 1028, 1031, 1035, 1036, 1038, 1045, 1048, 1049, 1052, 1056, 1058, 1060, 1063, 1067, 1069, 1071, 1075, 1076, 1077, 1080, 1086, 1095, 1096, 1097, 1101, 1107, 1110, 1111, 1112, 1123, 1130, 1135, 1139, 1143, 1145, 1146, 1147, 1151, 1153, 1157, 1158, 1161, 1164, 1165, 1166, 1168, 1169, 1174, 1175, 1176, 1182, 1183, 1184, 1187, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1209, 1211, 1212, 1216, 1220, 1227, 1231, 1238, 1245, 1247, 1248, 1249, 1252, 1256, 1257, 1259, 1261, 1264, 1271, 1275, 1277, 1282, 1288, 1293, 1295, 1298, 1299, 1303, 1304, 1309, 1310, 1312, 1318, 1319, 1321, 1322, 1326, 1327, 1333, 1335, 1337, 1338, 1339, 1343, 1345, 1346, 1351, 1352, 1353, 1355, 1358, 1359, 1360, 1363, 1375, 1378, 1379, 1380, 1381, 1382, 1385, 1386, 1390, 1396, 1397, 1399, 1400, 1401, 1407, 1410, 1413, 1423, 1425, 1426, 1427, 1429, 1430, 1436, 1437, 1438, 1440, 1446, 1452, 1453, 1459, 1462, 1464, 1465, 1466, 1468, 1469, 1471, 1472, 1475, 1477, 1480, 1481, 1490, 1492, 1494, 1495, 1498, 1499, 1501, 1503, 1504, 1505, 1509, 1515, 1516, 1523, 1524, 1530, 1532, 1539, 1540, 1543, 1545, 1550, 1552, 1553, 1560, 1565, 1566, 1567, 1568, 1569, 1571, 1574, 1576, 1578, 1580, 1584, 1588, 1589, 1590, 1596, 1601, 1604, 1610, 1611, 1612, 1613, 1614, 1617, 1621, 1624, 1634, 1638, 1639, 1641, 1647, 1648, 1651, 1665, 1666, 1668, 1669, 1673, 1674, 1678, 1680, 1684, 1685, 1687, 1689, 1699, 1701, 1702, 1707, 1711, 1713, 1716, 1717, 1720, 1721, 1724, 1725, 1726, 1727, 1728, 1729, 1732, 1735, 1738, 1739, 1748, 1754, 1755, 1757, 1760, 1761, 1762, 1763, 1771, 1775, 1776, 1778, 1781, 1787, 1788, 1789, 1798, 1799, 1800, 1801, 1803, 1808, 1812, 1817, 1818, 1823, 1824, 1830, 1831, 1832, 1837, 1838, 1846, 1848, 1851, 1853, 1857, 1860, 1861, 1862, 1866, 1870, 1871, 1873, 1874, 1875, 1876, 1889, 1892, 1895, 1897, 1898, 1899, 1905, 1906, 1912, 1915, 1918, 1924, 1925, 1930, 1942, 1945, 1949, 1953, 1960, 1962, 1964, 1974, 1977, 1980, 1981, 1994, 1996, 1997, 1998, 2000, 2004, 2009, 2012, 2015, 2018, 2031, 2032, 2033, 2035, 2038, 2047]

idx_test4=[0, 516, 2057, 10, 2061, 2062, 1038, 1040, 1553, 23, 24, 1531, 2072, 27, 1054, 30, 1569, 34, 36, 37, 1573, 1063, 41, 2090, 2091, 1069, 1584, 50, 1075, 1079, 55, 62, 63, 65, 66, 1604, 71, 80, 595, 1620, 601, 602, 98, 1635, 614, 1127, 105, 618, 619, 620, 1131, 1130, 624, 109, 622, 631, 632, 1659, 1661, 1149, 640, 1665, 1152, 644, 645, 1159, 1160, 651, 1166, 655, 1680, 1171, 660, 1684, 1687, 1691, 1694, 1191, 167, 1194, 1706, 1196, 1711, 688, 1207, 188, 1216, 707, 1731, 1737, 205, 718, 207, 1744, 1750, 1752, 218, 220, 737, 1250, 740, 1770, 759, 760, 1273, 2085, 1791, 1794, 1285, 262, 1288, 786, 784, 791, 283, 796, 800, 803, 1829, 807, 1328, 1329, 1844, 1333, 823, 1337, 1338, 1339, 831, 832, 833, 838, 1352, 857, 858, 1882, 860, 1887, 1377, 1378, 1894, 1388, 1390, 879, 1392, 1904, 1905, 1403, 1404, 1920, 899, 904, 1417, 914, 915, 1945, 926, 1950, 1439, 1954, 931, 1445, 938, 1451, 1963, 1453, 1454, 1969, 1971, 949, 438, 1463, 953, 1980, 1982, 1983, 451, 452, 1481, 973, 979, 983, 1496, 987, 988, 989, 485, 2021, 1001, 1517, 1018, 1019, 1021]

idx_test5=[1543, 2056, 1031, 1035, 1037, 1038, 17, 530, 531, 2069, 2071, 1560, 25, 2079, 2080, 1569, 546, 2083, 2085, 1571, 551, 1576, 553, 1067, 2094, 559, 1071, 48, 564, 565, 1596, 2109, 2108, 575, 1098, 1099, 1613, 78, 592, 593, 594, 1110, 1621, 1631, 96, 1634, 613, 103, 1130, 106, 618, 111, 625, 1146, 124, 1151, 646, 1673, 1674, 1678, 144, 1172, 1685, 1687, 661, 157, 1699, 1193, 1194, 1195, 171, 1196, 1197, 175, 176, 181, 188, 1726, 1727, 704, 193, 192, 198, 712, 710, 1735, 203, 208, 1748, 1237, 1238, 727, 215, 1755, 220, 1249, 740, 232, 1369, 1259, 752, 756, 1782, 246, 247, 1788, 764, 1796, 1797, 1798, 1803, 1293, 1806, 1295, 1297, 274, 1298, 1815, 791, 1817, 1310, 1312, 805, 1830, 807, 300, 305, 819, 314, 1851, 1853, 1856, 1345, 1862, 1351, 1866, 1871, 1875, 852, 341, 1876, 853, 854, 1363, 1370, 1880, 860, 1881, 346, 1375, 857, 1889, 354, 865, 356, 1382, 1385, 877, 878, 1390, 880, 370, 884, 1909, 1400, 376, 1917, 1925, 390, 392, 1936, 1938, 403, 1425, 406, 1947, 923, 924, 1438, 1440, 929, 422, 426, 1451, 1452, 1965, 431, 1973, 436, 440, 1465, 1466, 1979, 1468, 1471, 450, 1475, 456, 1481, 970, 1480, 1998, 465, 980, 1494, 984, 1492, 987, 480, 1007, 496, 1521, 1523, 1018, 507]

idx_test6=[0, 1, 2052, 6, 8, 2062, 21, 23, 24, 2076, 2080, 2081, 2083, 37, 2087, 41, 2090, 2091, 47, 55, 71, 76, 80, 81, 83, 89, 90, 91, 97, 114, 122, 127, 129, 132, 139, 141, 144, 171, 187, 198, 205, 220, 225, 247, 257, 262, 283, 303, 308, 317, 321, 327, 390, 401, 430, 433, 437, 451, 452, 498, 516, 524, 537, 601, 604, 616, 618, 619, 620, 623, 628, 631, 632, 636, 640, 644, 645, 655, 660, 662, 671, 687, 688, 700, 716, 721, 728, 741, 759, 760, 765, 786, 800, 803, 807, 823, 824, 831, 832, 839, 840, 850, 854, 857, 858, 859, 860, 864, 868, 876, 887, 904, 908, 915, 916, 927, 938, 945, 946, 950, 962, 966, 974, 979, 988, 999, 1007, 1021, 1027, 1038, 1040, 1051, 1054, 1070, 1079, 1093, 1125, 1128, 1129, 1149, 1150, 1160, 1164, 1166, 1171, 1191, 1194, 1195, 1220, 1225, 1242, 1245, 1271, 1272, 1277, 1284, 1285, 1288, 1295, 1297, 1329, 1333, 1338, 1341, 1352, 1355, 1367, 1377, 1378, 1381, 1387, 1388, 1390, 1392, 1396, 1403, 1409, 1410, 1424, 1428, 1432, 1433, 1437, 1439, 1447, 1448, 1451, 1453, 1454, 1458, 1462, 1463, 1481, 1499, 1525, 1530, 1531, 1543, 1550, 1553, 1555, 1564, 1569, 1591, 1606, 1611, 1617, 1618, 1620, 1630, 1635, 1636, 1644, 1648, 1665, 1676, 1681, 1694, 1695, 1703, 1706, 1710, 1716, 1721, 1731, 1750, 1755, 1767, 1769, 1770, 1771, 1774, 1793, 1795, 1800, 1813, 1829, 1834, 1851, 1904, 1914, 1917, 1933, 1943, 1945, 1950, 1954, 1963, 1969, 1974, 1980, 1982, 1993, 1997, 2021]

idx_test7=[0, 1, 2052, 6, 8, 21, 24, 2076, 37, 2087, 41, 2090, 2091, 47, 55, 76, 81, 89, 90, 122, 127, 129, 132, 141, 187, 205, 220, 257, 262, 283, 303, 308, 317, 321, 437, 451, 452, 498, 516, 537, 601, 604, 616, 619, 620, 623, 632, 636, 640, 644, 655, 660, 662, 671, 687, 688, 700, 728, 741, 759, 760, 765, 800, 823, 824, 831, 832, 840, 858, 859, 860, 864, 868, 915, 927, 938, 945, 974, 979, 1027, 1040, 1051, 1054, 1070, 1079, 1093, 1125, 1128, 1129, 1149, 1150, 1160, 1171, 1225, 1242, 1272, 1284, 1285, 1297, 1329, 1341, 1367, 1377, 1387, 1388, 1392, 1403, 1409, 1424, 1432, 1433, 1439, 1447, 1448, 1451, 1454, 1458, 1463, 1525, 1531, 1555, 1564, 1591, 1606, 1618, 1620, 1630, 1635, 1636, 1644, 1676, 1681, 1694, 1695, 1703, 1706, 1710, 1731, 1750, 1767, 1769, 1770, 1774, 1793, 1795, 1813, 1829, 1834, 1904, 1914, 1917, 1933, 1943, 1950, 1954, 1963, 1969, 1982, 1993, 2021]
#print("adj.shape[0]",adj.shape[0])
#idx_test8=[0, 516, 2057, 10, 2061, 1040, 24, 1531, 27, 1054, 30, 36, 37, 1573, 41, 2090, 2091, 50, 1079, 55, 62, 65, 66, 1620, 601, 602, 1635, 1127, 105, 619, 620, 1131, 624, 109, 622, 632, 1659, 1661, 1149, 640, 1152, 644, 1159, 1160, 655, 1171, 660, 1691, 1694, 167, 1706, 688, 1207, 707, 1731, 1737, 205, 207, 1744, 1750, 1752, 220, 1250, 1770, 759, 760, 1273, 1791, 1794, 1285, 262, 784, 283, 796, 800, 1829, 1328, 1329, 1844, 823, 831, 832, 838, 858, 1882, 860, 1887, 1377, 1894, 1388, 1392, 1904, 1403, 1404, 1920, 899, 1417, 914, 915, 926, 1950, 1439, 1954, 1445, 938, 1451, 1963, 1454, 1969, 1971, 949, 438, 1463, 953, 1982, 1983, 451, 452, 973, 979, 983, 1496, 2021, 1001, 1517]

idx_test8=[2056, 1036, 1037, 527, 528, 1552, 530, 1045, 2071, 545, 548, 1576, 1571, 554, 1067, 551, 2093, 1071, 561, 562, 564, 1076, 567, 59, 572, 573, 1603, 582, 1098, 1613, 1621, 96, 1632, 103, 106, 1642, 1645, 625, 1146, 1662, 128, 1668, 646, 1673, 1674, 151, 1689, 1180, 1699, 1187, 1190, 1705, 684, 1199, 175, 1717, 181, 1209, 702, 1727, 1728, 1729, 194, 192, 704, 710, 706, 203, 1735, 711, 215, 1754, 1757, 1247, 1248, 1762, 1252, 233, 1259, 238, 752, 242, 1270, 1788, 1796, 1798, 776, 266, 1803, 1293, 1806, 1299, 1301, 1815, 1304, 1821, 287, 291, 805, 1318, 1831, 1832, 1830, 300, 1316, 1326, 304, 305, 1849, 314, 316, 1853, 1857, 1862, 1351, 843, 1871, 1875, 1363, 855, 346, 1374, 1889, 354, 1897, 1385, 877, 880, 370, 1909, 1399, 380, 382, 1413, 392, 393, 394, 1925, 911, 400, 404, 923, 1436, 418, 422, 937, 1452, 1965, 1460, 441, 1466, 1979, 444, 1468, 448, 455, 1998, 1487, 1488, 464, 978, 1490, 1494, 475, 1504, 2015, 503, 996, 487, 1000, 1828, 1520, 496, 1522, 1523, 2037, 501, 505, 507, 2047]

idx_test=np.array(idx_test8)

#idx_test=idx_test1

idx_unlabeled = np.union1d(idx_val, idx_test)





perturbations = int(args.ptb_rate * (adj.sum()//2))

adj, features, labels = preprocess(adj, features, labels, preprocess_adj=False)



# set up attack model
if 'Self' in args.model:
    lambda_ = 0
if 'Train' in args.model:
    lambda_ = 1
if 'Both' in args.model:
    lambda_ = 0.5

if 'A' in args.model:
    model = MetaApprox(nfeat=features.shape[1], hidden_sizes=[args.hidden],
                       nnodes=adj.shape[0], nclass=nclass, dropout=0.5,
                       train_iters=100, attack_features=False, lambda_=lambda_, device=device)

else:
    model = Metattack(nfeat=features.shape[1], hidden_sizes=[args.hidden],
                       nnodes=adj.shape[0], nclass=nclass, dropout=0.5,
                       train_iters=100, attack_features=False, lambda_=lambda_, device=device)

if device != 'cpu':
    adj = adj.to(device)
    features = features.to(device)
    labels = labels.to(device)
    model = model.to(device)


def test(adj):
    ''' test on HITSGNN '''

    adj = normalize_adj_tensor(adj)
    hitsgnn = HITSGNN(nfeat=features.shape[1],
              nhid=args.hidden,
              nclass=labels.max().item() + 1,
              dropout=0.5)

    if device != 'cpu':
        hitsgnn = hitsgnn.to(device)

    optimizer = optim.Adam(hitsgnn.parameters(),
                           lr=args.lr, weight_decay=5e-4)

    hitsgnn.train()

    for epoch in range(args.epochs):
        optimizer.zero_grad()
        output = hitsgnn(features, adj)
        loss_train = F.nll_loss(output[idx_train], labels[idx_train])
        acc_train = accuracy(output[idx_train], labels[idx_train])
        loss_train.backward()
        optimizer.step()

    hitsgnn.eval()
    output = hitsgnn(features, adj)


    #loss_test = F.nll_loss(output[idx_test], labels[idx_test])
    acc_test = accuracy(output[idx_test], labels[idx_test])


    # print("Test set results:",
    #       "loss= {:.4f}".format(loss_test.item()),
    #       "accuracy= {:.4f}".format(acc_test.item()))

    return acc_test.item()


def main():

    acc = []
    runs = 1
    
    for i in range(runs):
        acc.append(test(adj))

    torch.cuda.empty_cache()

    #for i in range(3):
    
    modified_adj = model(features, adj, labels, idx_train,
                            idx_unlabeled, perturbations, ll_constraint=False)
    modified_adj = modified_adj.detach()

    for i in range(runs):
        acc.append(test(modified_adj))

    print("acc",acc)



    #data=pd.DataFrame({"Acc. Clean":clean_acc,"Acc. Perturbed":attacked_acc})

    #plt.figure(figsize=(6,6))
    #sns.boxplot(data=data)#, re_trainings*[accuracy_logistic]])

    #plt.title("Accuracy before/after perturbing {}% edges using model {}".format(args.ptb_rate*100, args.model))
    #plt.savefig("results_on_{}.png".format(args.dataset), dpi=600)
    #plt.show()


if __name__ == '__main__':
    main()


#neighbor type 0:
#acc [0.7008293838862559, 0.5722748815165877, 0.4792654028436019, 0.43246445497630337]
#acc [0.7008293838862559, 0.571090047393365, 0.504739336492891, 0.5011848341232228]

#neighbor type 1:
#acc [0.7184466019417475, 0.6359223300970873, 0.6407766990291262, 0.635922330097087]
#acc [0.7184466019417475, 0.6213592233009708, 0.6067961165048543, 0.616504854368932]

#neighbor type 2:
#acc [0.6771428571428572, 0.5942857142857143,  0.5971428571428572,  0.5971428571428572]
#acc [0.6771428571428572, 0.6457142857142857, 0.6114285714285714, 0.6257142857142857]

#neighbor type 3:
#acc [0.7798833819241983, 0.6239067055393586, 0.575801749271137, 0.5043731778425656]
#acc [0.7798833819241983, 0.6428571428571428, 0.5626822157434402, 0.5830903790087464]

#neighbor type 4:
#acc [0.7305699481865285, 0.6787564766839379, 0.7046632124352332,  0.6994818652849741]
#acc [0.7305699481865285, 0.6787564766839379, 0.6217616580310881, 0.6217616580310881]

#neighbor type 5:
#acc [0.7488372093023256, 0.6883720930232557, 0.6883720930232557, 0.7116279069767442]
#acc [0.7488372093023256, 0.6790697674418604, 0.6186046511627907, 0.613953488372093]

#neighbor type 6:
#acc [0.7058823529411764, 0.6823529411764706, 0.6823529411764706, 0.6862745098039216]
#acc [0.7058823529411764, 0.6745098039215687, 0.6862745098039216, 0.6862745098039216]

#neighbor type 7:
#acc [0.65625, 0.6375000000000001, 0.64375, 0.65]

#neighbor type 8:
#acc [0.6880000000000001, 0.728, 0.6960000000000001, 0.712]

#neighbor type 8-0:
#acc [0.7111111111111111, 0.5555555555555556, 0.4666666666666667, 0.4777777777777778]
#acc [0.7111111111111111, 0.5944444444444444, 0.6111111111111112, 0.5833333333333334]

